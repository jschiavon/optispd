{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Simulation of a Riemannian optimization\n",
    "We simulate a dataset $\\left\\lbrace x_i\\right\\rbrace_i^n$ from a $p$-variate normal with mean $\\mu$ and variance-covariance matrix $\\Sigma$ and then we try to estimate both parameters via the maximization of the loglikelihood function\n",
    "$$ \\mathcal{L}(\\mu, \\Sigma) = \\sum_i^n\\log\\left[\\phi_{p}(x_i, \\mu, \\Sigma) \\right] $$\n",
    "where $\\phi_{p}(x, \\mu, \\Sigma)$ is the density function of a $p$-variate gaussian distribution of mean $\\mu$ and variance $\\Sigma$.\n",
    "\n",
    "We perform a reparametrization via data augmentation to obtain a convex function by expanding the data by adding a column of ones as $ y_i^\\top = \\left[x_i^\\top 1 \\right]$ and then we consider the expanded loglikelihood function\n",
    "$$ \\tilde{\\mathcal{L}}(\\Omega) = \\sum_i^n\\log\\left[\\sqrt{2\\pi}e^{1/2}\\phi_{p+1}(y_i, 0, \\Omega) \\right]. $$\n",
    "\n",
    "It can be proven that if $\\hat{\\Omega}$ is the maximizer for $\\tilde{\\mathcal{L}}(\\Omega)$ and $(\\hat\\mu, \\hat\\Sigma)$ is the maximizer for $\\mathcal{L}(\\mu,\\Sigma)$, then\n",
    "\n",
    "$$ \n",
    "\\hat\\Omega = \\begin{pmatrix} \n",
    "        \\hat\\Sigma + \\hat\\mu\\hat\\mu^\\top & \\hat\\mu \\\\\n",
    "        \\hat\\mu^\\top & 1 \n",
    "    \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T13:41:07.942Z",
     "iopub.status.busy": "2021-01-04T13:41:07.937Z",
     "iopub.status.idle": "2021-01-04T13:41:08.579Z",
     "shell.execute_reply": "2021-01-04T13:41:08.661Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, random, grad\n",
    "from jax.ops import index_update\n",
    "from jax.config import config\n",
    "config.update('jax_enable_x64', True)\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "seed = 42\n",
    "RNG = random.PRNGKey(seed)\n",
    "\n",
    "from libs.manifold import SPD\n",
    "from libs.minimizer import OPTIM\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(\"notebook\")\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [14, 7]\n",
    "# plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T13:41:08.595Z",
     "iopub.status.busy": "2021-01-04T13:41:08.589Z",
     "iopub.status.idle": "2021-01-04T13:41:08.605Z",
     "shell.execute_reply": "2021-01-04T13:41:08.667Z"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "p = 10\n",
    "_tol = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T13:41:08.621Z",
     "iopub.status.busy": "2021-01-04T13:41:08.614Z",
     "iopub.status.idle": "2021-01-04T13:41:08.632Z",
     "shell.execute_reply": "2021-01-04T13:41:08.671Z"
    }
   },
   "outputs": [],
   "source": [
    "orig_man = SPD(p)\n",
    "man = SPD(p + 1)\n",
    "optim_rsd = OPTIM(man, method='rsd', mingradnorm=_tol, verbosity=0, logverbosity=True)\n",
    "optim_rcg = OPTIM(man, method='rcg', mingradnorm=_tol, verbosity=0, logverbosity=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T13:41:08.647Z",
     "iopub.status.busy": "2021-01-04T13:41:08.642Z",
     "iopub.status.idle": "2021-01-04T13:41:09.644Z",
     "shell.execute_reply": "2021-01-04T13:41:09.649Z"
    }
   },
   "outputs": [],
   "source": [
    "RNG, key = random.split(RNG)\n",
    "t_cov = orig_man.rand(key)\n",
    "RNG, key = random.split(RNG)\n",
    "t_mean = random.normal(key, shape=(p,))\n",
    "RNG, key = random.split(RNG)\n",
    "data = random.multivariate_normal(key, mean=t_mean, cov=t_cov, shape=(n_samples,))\n",
    "s_mu = jnp.mean(data, axis=0)\n",
    "S = jnp.dot((data - s_mu).T, data - s_mu)\n",
    "s_cov = S / n_samples\n",
    "eigs = jnp.linalg.eigvalsh(s_cov)\n",
    "min_eig, max_eig = min(eigs), max(eigs)\n",
    "\n",
    "MLE = jnp.append(jnp.append(s_cov + jnp.outer(s_mu, s_mu), jnp.array([s_mu]), axis=0), \n",
    "                 jnp.array([jnp.append(s_mu, 1)]).T, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T13:41:09.689Z",
     "iopub.status.busy": "2021-01-04T13:41:09.684Z",
     "iopub.status.idle": "2021-01-04T13:41:10.363Z",
     "shell.execute_reply": "2021-01-04T13:41:10.436Z"
    }
   },
   "outputs": [],
   "source": [
    "def loglik(X):\n",
    "    y = jnp.concatenate([data.T, jnp.ones(shape=(1, n_samples))], axis=0)\n",
    "    S = jnp.dot(y, y.T)\n",
    "    return - 0.5 * (n_samples * jnp.linalg.slogdet(X)[1] + jnp.trace(jnp.linalg.solve(X, S)))\n",
    "\n",
    "fun_riem = jit(lambda X: - loglik(X))\n",
    "gra_riem = jit(grad(fun_riem))\n",
    "\n",
    "true_fun = fun_riem(MLE)\n",
    "true_gra = gra_riem(MLE)\n",
    "true_gra_norm = man.norm(MLE, true_gra)\n",
    "print(true_fun)\n",
    "print(true_gra_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T13:41:10.375Z",
     "iopub.status.busy": "2021-01-04T13:41:10.371Z",
     "iopub.status.idle": "2021-01-04T13:41:13.616Z",
     "shell.execute_reply": "2021-01-04T13:41:13.653Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in trange(2):\n",
    "    RNG, key = random.split(RNG)\n",
    "    init = man.rand(key)\n",
    "    \n",
    "    # print(optim_rsd)\n",
    "    results_rsd, log_rsd = optim_rsd.solve(fun_riem, gra_riem, x=init)\n",
    "    # print(results_rsd, '\\n')\n",
    "    \n",
    "    # print(optim_rcg)\n",
    "    results_rcg, log_rcg = optim_rcg.solve(fun_riem, gra_riem, x=init)\n",
    "    # print(results_rcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T13:41:13.638Z",
     "iopub.status.busy": "2021-01-04T13:41:13.629Z",
     "iopub.status.idle": "2021-01-04T13:41:15.187Z",
     "shell.execute_reply": "2021-01-04T13:41:15.285Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=(14, 21))\n",
    "\n",
    "ax1.plot(log_rsd.it, jnp.abs(log_rsd.fun - true_fun), label='r-sd');\n",
    "ax1.plot(log_rcg.it, jnp.abs(log_rcg.fun - true_fun), label='r-cg');\n",
    "ax1.axhline(y=_tol, xmin=0, xmax=max(results_rsd.nit, results_rcg.nit));\n",
    "ax1.set_yscale('log');\n",
    "ax1.set_ylabel(r'$\\log\\vert\\mathcal{L}_\\star - \\hat{\\mathcal{L}}\\vert$');\n",
    "\n",
    "ax2.plot(log_rsd.it, jnp.abs(log_rsd.grnorm - true_gra_norm), label='r-sd');\n",
    "ax2.plot(log_rcg.it, jnp.abs(log_rcg.grnorm - true_gra_norm), label='r-cg'); \n",
    "ax2.axhline(y=_tol, xmin=0, xmax=max(results_rsd.nit, results_rcg.nit));\n",
    "ax2.set_yscale('log');\n",
    "ax2.set_ylabel(r'$\\log\\left\\vert\\Vert\\nabla\\mathcal{L}_\\star\\Vert^2 - \\Vert\\nabla\\hat{\\mathcal{L}}\\Vert^2\\right\\vert$');\n",
    "\n",
    "ax3.plot(log_rsd.it, [man.dist(MLE, log_rsd.x[i]) for i in range(results_rsd.nit+1)], label='r-sd'); \n",
    "ax3.plot(log_rcg.it, [man.dist(MLE, log_rcg.x[i]) for i in range(results_rcg.nit+1)], label='r-cg');\n",
    "ax3.axhline(y=_tol, xmin=0, xmax=max(results_rsd.nit, results_rcg.nit));\n",
    "ax3.set_yscale('log');\n",
    "ax3.set_ylabel(r'$\\log\\left\\Vert\\hat\\Omega - \\Omega_\\star\\right\\Vert$');\n",
    "\n",
    "f.suptitle('Optimizers performances on {0} x {0} SPD manifold'.format(p));\n",
    "plt.xlabel('Iterations');\n",
    "plt.legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large simulation\n",
    "We simulate various datasets with varying dimensions and repeat the experiment multiple times from multiple starting points.\n",
    "\n",
    "**Warning**:\n",
    "\n",
    "It takes quite a while to perform all the simulations (around 30 minutes on my i7 laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T13:41:15.199Z",
     "iopub.status.busy": "2021-01-04T13:41:15.194Z",
     "iopub.status.idle": "2021-01-04T13:41:15.207Z",
     "shell.execute_reply": "2021-01-04T13:41:15.291Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "RNG = random.PRNGKey(seed)\n",
    "\n",
    "n_samples = 1000\n",
    "ps = [2, 3, 5]\n",
    "_tol = 1e-4\n",
    "\n",
    "n_test = 3\n",
    "n_reps = 3\n",
    "\n",
    "tot_len = len(ps) * n_test * n_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_dim = np.zeros(shape=(tot_len * 3), dtype=int)\n",
    "algo = ['R-SD', 'R-CG', 'Chol'] * tot_len\n",
    "times = np.zeros(shape=(tot_len * 3))\n",
    "iters = np.zeros(shape=(tot_len * 3), dtype=int)\n",
    "fun_diff = np.zeros(shape=(tot_len * 3))\n",
    "mat_diff = np.zeros(shape=(tot_len * 3))\n",
    "grnorm = np.zeros(shape=(tot_len * 3))\n",
    "i = 0\n",
    "\n",
    "for p in ps:\n",
    "    orig_man = SPD(p)\n",
    "    man = SPD(p + 1)\n",
    "    optim_rsd = OPTIM(man, method='rsd', maxiter=200, mingradnorm=_tol, verbosity=0)\n",
    "    optim_rcg = OPTIM(man, method='rcg', maxiter=200, mingradnorm=_tol, verbosity=0)\n",
    "    print(orig_man)\n",
    "    \n",
    "    for _ in trange(n_test):\n",
    "        RNG, key = random.split(RNG)\n",
    "        t_cov = orig_man.rand(key)\n",
    "        RNG, key = random.split(RNG)\n",
    "        t_mean = random.normal(key, shape=(p,))\n",
    "        RNG, key = random.split(RNG)\n",
    "        data = random.multivariate_normal(key, mean=t_mean, cov=t_cov, shape=(n_samples,))\n",
    "        s_mu = jnp.mean(data, axis=0)\n",
    "        S = jnp.dot((data - s_mu).T, data - s_mu)\n",
    "        s_cov = S / n_samples\n",
    "        eigs = jnp.linalg.eigvalsh(s_cov)\n",
    "        min_eig, max_eig = min(eigs), max(eigs)\n",
    "        \n",
    "        MLE = jnp.append(jnp.append(s_cov + jnp.outer(s_mu, s_mu), jnp.array([s_mu]), axis=0), \n",
    "                         jnp.array([jnp.append(s_mu, 1)]).T, axis=1)\n",
    "        MLE_chol = jnp.linalg.cholesky(MLE)\n",
    "        MLE_chol = MLE_chol.T[~(MLE_chol.T == 0.)].ravel()\n",
    "\n",
    "        def loglik(X):\n",
    "            y = jnp.concatenate([data.T, jnp.ones(shape=(1, n_samples))], axis=0)\n",
    "            S = jnp.dot(y, y.T)\n",
    "            return - 0.5 * (n_samples * jnp.linalg.slogdet(X)[1] + jnp.trace(jnp.linalg.solve(X, S)))\n",
    "        \n",
    "        def loglik_chol(X):\n",
    "            cov = index_update(\n",
    "                jnp.zeros(shape=(p+1, p+1)),\n",
    "                jnp.triu_indices(p+1),\n",
    "                X).T\n",
    "            logdet = 2 + jnp.sum(jnp.diag(cov))\n",
    "            y = jnp.concatenate([data.T, jnp.ones(shape=(1, n_samples))], axis=0)\n",
    "            sol = jnp.linalg.solve(cov, y)\n",
    "            return - 0.5 * (n_samples * logdet + jnp.einsum('ij,ij', sol, sol))\n",
    "            \n",
    "        fun_riem = jit(lambda X: - loglik(X))\n",
    "        gra_riem = jit(grad(fun_riem))\n",
    "        fun_chol = jit(lambda X: - loglik_chol(X))\n",
    "        gra_chol = jit(grad(fun_chol))\n",
    "\n",
    "        true_fun = fun_riem(MLE)\n",
    "        true_gra = gra_riem(MLE)\n",
    "        true_gra_norm = man.norm(MLE, true_gra)\n",
    "        true_fun_c = fun_chol(MLE_chol)\n",
    "        true_gra_c = gra_chol(MLE_chol)\n",
    "        true_gra_norm_c = jnp.linalg.norm(true_gra_c)\n",
    "        \n",
    "        for _ in range(n_reps):\n",
    "            RNG, key = random.split(RNG)\n",
    "            init = man.rand(key)\n",
    "            init_chol = random.normal(key, shape=MLE_chol.shape)\n",
    "            \n",
    "            for opt in [optim_rsd, optim_rcg]:\n",
    "                result = opt.solve(fun_riem, gra_riem, x=init)\n",
    "                mat_dim[i] = p\n",
    "                times[i] = float(result.time)\n",
    "                iters[i] = int(result.nit)\n",
    "                fun_diff[i] = float(result.fun - true_fun)\n",
    "                mat_diff[i] = float(man.dist(MLE, result.x))\n",
    "                grnorm[i] = float(result.grnorm)\n",
    "                i += 1\n",
    "            start = time()\n",
    "            result = minimize(fun_chol, init_chol, method='cg', jac=gra_chol, tol=_tol)\n",
    "            mat_dim[i] = p\n",
    "            times[i] = time() - start\n",
    "            iters[i] = result.nit\n",
    "            fun_diff[i] = result.fun - true_fun_c\n",
    "            ob_cov = index_update(\n",
    "                jnp.zeros(shape=(p+1, p+1)),\n",
    "                jnp.triu_indices(p+1),\n",
    "                result.x).T\n",
    "            mat_diff[i] = man.dist(jnp.matmul(ob_cov, ob_cov.T), MLE)\n",
    "            grnorm[i] = jnp.linalg.norm(result.jac)\n",
    "            i += 1\n",
    "            \n",
    "\n",
    "df = pd.DataFrame({'Matrix dimension': mat_dim, 'Algorithm': algo, \n",
    "        'Time': times, 'Iterations': iters, 'Time / Iteration': times / iters, \n",
    "        'Function difference': jnp.abs(fun_diff), 'Matrix distance': mat_diff, 'Gradient norm': grnorm})\n",
    "\n",
    "df['convergence'] = df['fun diff'] < _tol\n",
    "df.to_csv('sample_simulation_cholesky.csv', index=False)\n",
    "\n",
    "print('R-SD algorithm convergence ratio {:.2f} %'.format(len(df[df.convergence & (df.algorithm == 'R-SD')]) / tot_len * 100.))\n",
    "print('R-CG algorithm convergence ratio {:.2f} %'.format(len(df[df.convergence & (df.algorithm == 'R-CG')]) / tot_len * 100.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T14:04:35.105Z",
     "iopub.status.busy": "2021-01-04T14:04:35.096Z",
     "iopub.status.idle": "2021-01-04T14:04:36.472Z",
     "shell.execute_reply": "2021-01-04T14:04:36.479Z"
    }
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=(14, 21))\n",
    "\n",
    "sns.boxplot(data=df, x='Matrix dimension', y='Time', hue='Algorithm', ax=ax1)\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_xlabel('');\n",
    "ax1.set_ylabel('Time [s]');\n",
    "\n",
    "sns.boxplot(data=df, x='Matrix dimension', y='Matrix distance', hue='Algorithm', ax=ax2)\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xlabel('');\n",
    "ax2.set_ylabel(r'$\\log\\left\\Vert\\hat\\Omega - \\Omega_\\star\\right\\Vert$');\n",
    "\n",
    "sns.boxplot(data=df, x='Matrix dimension', y='Function difference', hue='Algorithm', ax=ax3)\n",
    "ax3.set_yscale('log')\n",
    "ax3.set_ylabel(r'$\\log\\vert\\mathcal{L}_\\star - \\hat{\\mathcal{L}}\\vert$');\n",
    "\n",
    "f.suptitle(r'Optimizers performances for $p$-variate normal');\n",
    "plt.xlabel(r'Matrix size $p$');\n",
    "plt.legend();\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T13:41:15.220Z",
     "iopub.status.busy": "2021-01-04T13:41:15.215Z",
     "iopub.status.idle": "2021-01-04T14:03:38.391Z",
     "shell.execute_reply": "2021-01-04T14:03:38.432Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "RNG = random.PRNGKey(seed)\n",
    "\n",
    "n_samples = 1000\n",
    "ps = [3, 5, 10, 15, 25, 50]\n",
    "_tol = 1e-4\n",
    "\n",
    "n_test = 10\n",
    "n_reps = 10\n",
    "\n",
    "tot_len = len(ps) * n_test * n_reps\n",
    "\n",
    "mat_dim = np.zeros(shape=(tot_len * 2), dtype=int)\n",
    "algo = ['R-SD', 'R-CG'] * tot_len\n",
    "times = np.zeros(shape=(tot_len * 2))\n",
    "iters = np.zeros(shape=(tot_len * 2), dtype=int)\n",
    "fun_diff = np.zeros(shape=(tot_len * 2))\n",
    "mat_diff = np.zeros(shape=(tot_len * 2))\n",
    "grnorm = np.zeros(shape=(tot_len * 2))\n",
    "i = 0\n",
    "\n",
    "for p in ps:\n",
    "    orig_man = SPD(p)\n",
    "    man = SPD(p + 1)\n",
    "    optim_rsd = OPTIM(man, method='rsd', maxiter=200, mingradnorm=_tol, verbosity=0)\n",
    "    optim_rcg = OPTIM(man, method='rcg', maxiter=200, mingradnorm=_tol, verbosity=0)\n",
    "    print(orig_man)\n",
    "    \n",
    "    for _ in trange(n_test):\n",
    "        RNG, key = random.split(RNG)\n",
    "        t_cov = orig_man.rand(key)\n",
    "        RNG, key = random.split(RNG)\n",
    "        t_mean = random.normal(key, shape=(p,))\n",
    "        RNG, key = random.split(RNG)\n",
    "        data = random.multivariate_normal(key, mean=t_mean, cov=t_cov, shape=(n_samples,))\n",
    "        s_mu = jnp.mean(data, axis=0)\n",
    "        S = jnp.dot((data - s_mu).T, data - s_mu)\n",
    "        s_cov = S / n_samples\n",
    "        eigs = jnp.linalg.eigvalsh(s_cov)\n",
    "        min_eig, max_eig = min(eigs), max(eigs)\n",
    "\n",
    "        MLE = jnp.append(jnp.append(s_cov + jnp.outer(s_mu, s_mu), jnp.array([s_mu]), axis=0), \n",
    "                         jnp.array([jnp.append(s_mu, 1)]).T, axis=1)\n",
    "        \n",
    "        def loglik(X):\n",
    "            y = jnp.concatenate([data.T, jnp.ones(shape=(1, n_samples))], axis=0)\n",
    "            S = jnp.dot(y, y.T)\n",
    "            return - 0.5 * (n_samples * jnp.linalg.slogdet(X)[1] + jnp.trace(jnp.linalg.solve(X, S)))\n",
    "\n",
    "        fun_riem = jit(lambda X: - loglik(X))\n",
    "        gra_riem = jit(grad(fun_riem))\n",
    "\n",
    "        true_fun = fun_riem(MLE)\n",
    "        true_gra = gra_riem(MLE)\n",
    "        true_gra_norm = man.norm(MLE, true_gra)\n",
    "        \n",
    "        for _ in range(n_reps):\n",
    "            RNG, key = random.split(RNG)\n",
    "            init = man.rand(key)\n",
    "            \n",
    "            for opt in [optim_rsd, optim_rcg]:\n",
    "                result = opt.solve(fun_riem, gra_riem, x=init)\n",
    "                mat_dim[i] = p\n",
    "                times[i] = float(result.time)\n",
    "                iters[i] = int(result.nit)\n",
    "                fun_diff[i] = float(result.fun - true_fun)\n",
    "                mat_diff[i] = float(man.dist(MLE, result.x))\n",
    "                grnorm[i] = float(result.grnorm)\n",
    "                i += 1\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Matrix dimension': mat_dim, 'Algorithm': algo, \n",
    "        'Time': times, 'Iterations': iters, 'Time / Iteration': times / iters, \n",
    "        'Function difference': jnp.abs(fun_diff), 'Matrix distance': mat_diff, 'Gradient norm': grnorm})\n",
    "\n",
    "df['convergence'] = df['fun diff'] < _tol\n",
    "df.to_csv('sample_simulation_cholesky.csv', index=False)\n",
    "\n",
    "print('R-SD algorithm convergence ratio {:.2f} %'.format(len(df[df.convergence & (df.algorithm == 'R-SD')]) / tot_len * 100.))\n",
    "print('R-CG algorithm convergence ratio {:.2f} %'.format(len(df[df.convergence & (df.algorithm == 'R-CG')]) / tot_len * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import numpy as np\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, random, grad\n",
    "from jax.config import config\n",
    "config.update('jax_enable_x64', True)\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "seed = 42\n",
    "RNG = random.PRNGKey(seed)\n",
    "\n",
    "from libs.manifold import SPD\n",
    "from libs.minimizer import OPTIM\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(\"notebook\")\n",
    "n_samples = 1000\n",
    "p = 10\n",
    "_tol = 1e-6\n",
    "orig_man = SPD(p)\n",
    "man = SPD(p + 1)\n",
    "optim_rsd = OPTIM(man, method='rsd', mingradnorm=_tol, verbosity=1, logverbosity=True)\n",
    "optim_rcg = OPTIM(man, method='rcg', mingradnorm=_tol, verbosity=1, logverbosity=True)\n",
    "RNG, key = random.split(RNG)\n",
    "t_cov = orig_man.rand(key)\n",
    "RNG, key = random.split(RNG)\n",
    "t_mean = random.normal(key, shape=(p,))\n",
    "RNG, key = random.split(RNG)\n",
    "data = random.multivariate_normal(key, mean=t_mean, cov=t_cov, shape=(n_samples,))\n",
    "s_mu = jnp.mean(data, axis=0)\n",
    "S = jnp.dot((data - s_mu).T, data - s_mu)\n",
    "s_cov = S / n_samples\n",
    "eigs = jnp.linalg.eigvalsh(s_cov)\n",
    "min_eig, max_eig = min(eigs), max(eigs)\n",
    "\n",
    "MLE = jnp.append(jnp.append(s_cov + jnp.outer(s_mu, s_mu), jnp.array([s_mu]), axis=0),\n",
    "                 jnp.array([jnp.append(s_mu, 1)]).T, axis=1)\n",
    "def loglik(X):\n",
    "    y = jnp.concatenate([data.T, jnp.ones(shape=(1, n_samples))], axis=0)\n",
    "    S = jnp.dot(y, y.T)\n",
    "    return - 0.5 * (n_samples * jnp.linalg.slogdet(X)[1] + jnp.trace(jnp.linalg.solve(X, S)))\n",
    "\n",
    "fun_riem = jit(lambda X: - loglik(X))\n",
    "gra_riem = jit(grad(fun_riem))\n",
    "\n",
    "true_fun = fun_riem(MLE)\n",
    "true_gra = gra_riem(MLE)\n",
    "true_gra_norm = man.norm(MLE, true_gra)\n",
    "print(true_fun)\n",
    "print(true_gra_norm)\n",
    "\n",
    "RNG, key = random.split(RNG)\n",
    "init = man.rand(key)\n",
    "results_rsd, log_rsd = optim_rsd.solve(fun_riem, gra_riem, x=init)\n",
    "results_rcg, log_rcg = optim_rcg.test_solver(fun_riem, gra_riem, x=init)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('pyintel': conda)",
   "language": "python",
   "name": "python37764bitpyintelcondac1f6f4818b104bbfb9da35628e896f42"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nteract": {
   "version": "0.27.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
